{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8f0da4",
   "metadata": {},
   "source": [
    "# Module 3 – Case Study Mini-Project\n",
    "\n",
    "In this notebook I go through a full data preparation pipeline for a **retail store sales** dataset.\n",
    "The goal is to clean and transform the data so that it is ready for a future machine learning model\n",
    "that could predict the **total amount spent per transaction**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63551a59",
   "metadata": {},
   "source": [
    "## Part 0 – Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I import the main libraries I usually use for this kind of analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I load the raw retail store sales data\n",
    "# I also tell pandas that '?' should be treated as a missing value just in case\n",
    "csv_file = 'retail_store_sales_3b2f9e32-25a6-4de3-a38a-efafad9ca724.csv'\n",
    "df = pd.read_csv(csv_file, na_values='?')\n",
    "\n",
    "# I quickly check the shape so I know how many rows and columns I am working with\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29dbda5",
   "metadata": {},
   "source": [
    "## Part 1 – Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95597f9f",
   "metadata": {},
   "source": [
    "### 1.1 Initial Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to see a basic overview of the dataframe structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I check the first few rows to understand what the columns look like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf61dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use describe() to get summary statistics for the numerical features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And I also look at a describe with include='object' to see the categorical side\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969c687",
   "metadata": {},
   "source": [
    "### 1.2 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98230de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I pick a few important numerical columns and plot histograms\n",
    "num_cols = ['Price Per Unit', 'Quantity', 'Total Spent']\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    df[col].hist(bins=30)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6eb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do the same for some key categorical variables using bar plots\n",
    "cat_cols = ['Category', 'Payment Method', 'Location']\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, col in enumerate(cat_cols, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d3739",
   "metadata": {},
   "source": [
    "### 1.3 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I look at the relationship between Quantity and Total Spent with a scatter plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df['Quantity'], df['Total Spent'], alpha=0.3)\n",
    "plt.xlabel('Quantity')\n",
    "plt.ylabel('Total Spent')\n",
    "plt.title('Quantity vs Total Spent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daca2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also check the correlation between the numerical columns\n",
    "numeric_df = df[['Price Per Unit', 'Quantity', 'Total Spent']]\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.title('Correlation Heatmap (Numerical Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdca662",
   "metadata": {},
   "source": [
    "### 1.4 EDA Summary\n",
    "\n",
    "From the first exploration I notice the following points:\n",
    "\n",
    "1. There are **missing values** in several columns such as `Item`, `Price Per Unit`, `Quantity`, `Total Spent` and especially `Discount Applied`.\n",
    "2. The numerical variables (especially `Total Spent`) are **right-skewed**, which is expected because most transactions are small and a few are very large.\n",
    "3. `Quantity` and `Total Spent` show a clear **positive relationship**, as larger quantities usually lead to larger total amounts.\n",
    "4. Some payment methods and locations are clearly more frequent than others, which might influence the total spend.\n",
    "5. The dataset already has a reasonable set of features, but it will still benefit from some extra feature engineering around dates and locations later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4de5d",
   "metadata": {},
   "source": [
    "## Part 2 – Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3e52",
   "metadata": {},
   "source": [
    "### 2.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6658e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I check how many missing values I have in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16aeae9",
   "metadata": {},
   "source": [
    "For the missing values I use the following strategy:\n",
    "\n",
    "- For **`Item`** (categorical), I fill missing values with the **most frequent item** (mode).\n",
    "- For **`Discount Applied`** (categorical), I treat missing values as a separate category `'Unknown'`.\n",
    "- For the numerical trio **`Price Per Unit`**, **`Quantity`**, and **`Total Spent`**, I first try to\n",
    "  **reconstruct the missing value using the other two** (because `Total Spent = Price Per Unit * Quantity`).\n",
    "  If I still have missing values after that, I fill them with the **median**, because the distributions\n",
    "  are skewed and I do not want extreme values to influence the imputation too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I copy the dataframe so I keep the original version intact\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1) Fill missing Item with the most frequent value\n",
    "most_frequent_item = df_clean['Item'].mode()[0]\n",
    "df_clean['Item'] = df_clean['Item'].fillna(most_frequent_item)\n",
    "\n",
    "# 2) Treat missing Discount Applied as 'Unknown'\n",
    "df_clean['Discount Applied'] = df_clean['Discount Applied'].fillna('Unknown')\n",
    "\n",
    "# 3) Reconstruct missing numerical values if possible\n",
    "# If Price Per Unit is missing but Quantity and Total Spent are present\n",
    "mask_price_missing = df_clean['Price Per Unit'].isna() & df_clean['Quantity'].notna() & df_clean['Total Spent'].notna()\n",
    "df_clean.loc[mask_price_missing, 'Price Per Unit'] = (\n",
    "    df_clean.loc[mask_price_missing, 'Total Spent'] / df_clean.loc[mask_price_missing, 'Quantity']\n",
    ")\n",
    "\n",
    "# If Quantity is missing but Price Per Unit and Total Spent are present\n",
    "mask_quantity_missing = df_clean['Quantity'].isna() & df_clean['Price Per Unit'].notna() & df_clean['Total Spent'].notna()\n",
    "df_clean.loc[mask_quantity_missing, 'Quantity'] = (\n",
    "    df_clean.loc[mask_quantity_missing, 'Total Spent'] / df_clean.loc[mask_quantity_missing, 'Price Per Unit']\n",
    ")\n",
    "\n",
    "# If Total Spent is missing but Price Per Unit and Quantity are present\n",
    "mask_total_missing = df_clean['Total Spent'].isna() & df_clean['Price Per Unit'].notna() & df_clean['Quantity'].notna()\n",
    "df_clean.loc[mask_total_missing, 'Total Spent'] = (\n",
    "    df_clean.loc[mask_total_missing, 'Price Per Unit'] * df_clean.loc[mask_total_missing, 'Quantity']\n",
    ")\n",
    "\n",
    "# After reconstruction, I still fill any remaining numeric missing values with the median\n",
    "for col in ['Price Per Unit', 'Quantity', 'Total Spent']:\n",
    "    median_value = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_value)\n",
    "\n",
    "# I double-check that I do not have missing values left in these key columns\n",
    "df_clean[['Item', 'Price Per Unit', 'Quantity', 'Total Spent', 'Discount Applied']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95824e27",
   "metadata": {},
   "source": [
    "### 2.2 Data Type Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I convert Transaction Date to a proper datetime type\n",
    "df_clean['Transaction Date'] = pd.to_datetime(df_clean['Transaction Date'], errors='coerce')\n",
    "\n",
    "# I can also treat some object columns as categories\n",
    "categorical_columns = ['Transaction ID', 'Customer ID', 'Category', 'Item',\n",
    "                       'Payment Method', 'Location', 'Discount Applied']\n",
    "for col in categorical_columns:\n",
    "    df_clean[col] = df_clean[col].astype('category')\n",
    "\n",
    "df_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967c956",
   "metadata": {},
   "source": [
    "### 2.3 Outliers (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I quickly check for outliers in Total Spent using a boxplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.boxplot(df_clean['Total Spent'])\n",
    "plt.title('Boxplot of Total Spent')\n",
    "plt.ylabel('Total Spent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implement a simple IQR-based capping to reduce the impact of extreme outliers\n",
    "Q1 = df_clean['Total Spent'].quantile(0.25)\n",
    "Q3 = df_clean['Total Spent'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# I cap the values outside the bounds\n",
    "df_clean['Total Spent'] = np.where(\n",
    "    df_clean['Total Spent'] < lower_bound, lower_bound,\n",
    "    np.where(df_clean['Total Spent'] > upper_bound, upper_bound, df_clean['Total Spent'])\n",
    ")\n",
    "\n",
    "# I check the boxplot again after capping\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.boxplot(df_clean['Total Spent'])\n",
    "plt.title('Boxplot of Total Spent (After Capping)')\n",
    "plt.ylabel('Total Spent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29488a",
   "metadata": {},
   "source": [
    "## Part 3 – Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16f24c",
   "metadata": {},
   "source": [
    "I create at least two new features that might be useful for a future model:\n",
    "\n",
    "1. **`Transaction_Month`** – the month of the transaction (1–12). This can capture seasonality or monthly patterns.\n",
    "2. **`Is_Online`** – a binary feature that is 1 if the location is `'Online'` and 0 otherwise. This can capture\n",
    "   the difference between online and offline sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1895e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create the new features based on the cleaned dataframe\n",
    "df_fe = df_clean.copy()\n",
    "\n",
    "# Month of the transaction (I fill NaT just in case there were parsing issues)\n",
    "df_fe['Transaction_Month'] = df_fe['Transaction Date'].dt.month.fillna(df_fe['Transaction Date'].dt.month.mode()[0])\n",
    "\n",
    "# Binary feature: 1 if Online, 0 otherwise\n",
    "df_fe['Is_Online'] = (df_fe['Location'] == 'Online').astype(int)\n",
    "\n",
    "df_fe[['Transaction Date', 'Transaction_Month', 'Location', 'Is_Online']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eafcaa",
   "metadata": {},
   "source": [
    "## Part 4 – Data Transformation (Model-Ready Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef586c",
   "metadata": {},
   "source": [
    "For the model-ready dataset I:\n",
    "\n",
    "- Choose **`Total Spent`** as the target variable (this represents how much the customer paid).\n",
    "- Keep **numerical features** like `Price Per Unit`, `Quantity` and the engineered `Transaction_Month`.\n",
    "- Keep **categorical features** such as `Category`, `Item`, `Payment Method`, `Location`, and `Discount Applied`,\n",
    "  plus the binary `Is_Online` feature.\n",
    "- Apply **one-hot encoding** to the categorical features.\n",
    "- Use **StandardScaler** to scale the main numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288357d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# I define my target and the features I want to keep\n",
    "target_col = 'Total Spent'\n",
    "\n",
    "numeric_features = ['Price Per Unit', 'Quantity', 'Transaction_Month']\n",
    "categorical_features = ['Category', 'Item', 'Payment Method', 'Location', 'Discount Applied']\n",
    "binary_features = ['Is_Online']\n",
    "\n",
    "selected_columns = numeric_features + categorical_features + binary_features + [target_col]\n",
    "model_df = df_fe[selected_columns].copy()\n",
    "\n",
    "# I separate features and target\n",
    "X = model_df.drop(columns=[target_col])\n",
    "y = model_df[target_col]\n",
    "\n",
    "# I apply one-hot encoding to all categorical columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# I scale only the main continuous numeric features\n",
    "scaler = StandardScaler()\n",
    "X_encoded[numeric_features] = scaler.fit_transform(X_encoded[numeric_features])\n",
    "\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db38d6d",
   "metadata": {},
   "source": [
    "### 4.1 Final Model-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ecf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I combine the final feature matrix and the target into a single dataframe for inspection\n",
    "final_df = X_encoded.copy()\n",
    "final_df[target_col] = y\n",
    "\n",
    "# I check the first few rows of the fully numerical dataset\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518a52e",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "- The dataset is now cleaned, all important missing values are handled.\n",
    "- Outliers in `Total Spent` are capped to reduce their influence.\n",
    "- I created useful new features related to transaction month and online vs offline sales.\n",
    "- All categorical columns are encoded and the main numerical columns are scaled.\n",
    "\n",
    "This notebook can now be used as a base for building and testing machine learning models\n",
    "for predicting the total amount spent per transaction."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
